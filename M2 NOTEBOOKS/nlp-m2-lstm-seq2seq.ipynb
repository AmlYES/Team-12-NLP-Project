{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1246668,"sourceType":"datasetVersion","datasetId":715814}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load SQuAD dataset\ndataset = load_dataset(\"squad\", split=\"train\")\n\n# Step 1: Add a new column for context length\ndataset = dataset.map(lambda x: {\"context_length\": len(x[\"context\"])})\n\n# Step 2: Sort by the new column\nsorted_dataset = dataset.sort(\"context_length\")\n\n# Step 3: Select the shortest 10k rows (or whatever range you need)\nsubset = sorted_dataset.select(range(11000))  # You can change this to 5000, 20000, etc.\n\n# Preview\nprint(subset[0])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:29.884621Z","iopub.execute_input":"2025-04-22T18:01:29.884922Z","iopub.status.idle":"2025-04-22T18:01:31.305783Z","shell.execute_reply.started":"2025-04-22T18:01:29.884889Z","shell.execute_reply":"2025-04-22T18:01:31.305078Z"}},"outputs":[{"name":"stdout","text":"{'id': '56e10a3be3433e1400422b22', 'title': 'Space_Race', 'context': 'Meanwhile, the USSR continued briefly trying to perfect their N1 rocket, finally canceling it in 1976, after two more launch failures in 1971 and 1972.', 'question': \"Which year did the USSR cancel the N1 rocket program after two failures that didn't launch?\", 'answers': {'text': ['1976'], 'answer_start': [97]}, 'context_length': 151}\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Preview\nprint(subset[20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:31.307096Z","iopub.execute_input":"2025-04-22T18:01:31.307346Z","iopub.status.idle":"2025-04-22T18:01:31.312068Z","shell.execute_reply.started":"2025-04-22T18:01:31.307327Z","shell.execute_reply":"2025-04-22T18:01:31.311287Z"}},"outputs":[{"name":"stdout","text":"{'id': '56e3c2db39bdeb14003478f6', 'title': 'Estonian_language', 'context': 'From 1525 to 1917 14,503 titles were published in Estonian, as opposed to the 23,868 titles which were published between 1918 and 1940.[citation needed]', 'question': 'In what language were 14,503 books published prior to 1918?', 'answers': {'text': ['Estonian'], 'answer_start': [50]}, 'context_length': 152}\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import pandas as pd\n\n# Convert to pandas DataFrame\ndf = subset.to_pandas()\n\n# Show the first 20 rows\nprint(df.head(1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:31.312977Z","iopub.execute_input":"2025-04-22T18:01:31.313275Z","iopub.status.idle":"2025-04-22T18:01:31.560025Z","shell.execute_reply.started":"2025-04-22T18:01:31.313252Z","shell.execute_reply":"2025-04-22T18:01:31.558953Z"}},"outputs":[{"name":"stdout","text":"                         id       title  \\\n0  56e10a3be3433e1400422b22  Space_Race   \n\n                                             context  \\\n0  Meanwhile, the USSR continued briefly trying t...   \n\n                                            question  \\\n0  Which year did the USSR cancel the N1 rocket p...   \n\n                                    answers  context_length  \n0  {'text': ['1976'], 'answer_start': [97]}             151  \n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport ast\n\n# df['answers'] = df['answers'].apply(ast.literal_eval)\n\n\n# Extract the first answer text from the nested dictionary\ndf['answer_text'] = df['answers'].apply(lambda x: f\"<SOS> {x['text'][0]} <EOS>\")\n\n# Combine all text fields\nall_text = df['context'].tolist() + df['question'].tolist() + df['answer_text'].tolist()\n\n# Initialize tokenizer\ntokenizer = Tokenizer(\n    oov_token=\"<unk>\",\n    lower=False,\n    filters='!\"#$%&()*+,-./:;=@[\\\\]^_`{|}~\\t\\n'  # removed < and >\n\n\n)\n\ntokenizer.fit_on_texts(all_text)\n\n# Convert text to sequences\ncontext_seq = tokenizer.texts_to_sequences(df['context'].tolist())\nquestion_seq = tokenizer.texts_to_sequences(df['question'].tolist())\nanswer_seq = tokenizer.texts_to_sequences(df['answer_text'].tolist())\n\n# Pad the sequences\nmax_context_len = max(len(seq) for seq in context_seq)\nmax_question_len =max(len(seq) for seq in question_seq)\nmax_answer_len = max(len(seq) for seq in answer_seq)\n\ncontext_seq = pad_sequences(context_seq, maxlen=max_context_len, padding='post', truncating='post')\nquestion_seq = pad_sequences(question_seq, maxlen=max_question_len, padding='post', truncating='post')\nanswer_seq = pad_sequences(answer_seq, maxlen=max_answer_len, padding='post', truncating='post')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:31.561032Z","iopub.execute_input":"2025-04-22T18:01:31.561637Z","iopub.status.idle":"2025-04-22T18:01:32.802937Z","shell.execute_reply.started":"2025-04-22T18:01:31.561610Z","shell.execute_reply":"2025-04-22T18:01:32.802353Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"import numpy as np\n\n# Top 10 tokenized and padded sequences for context, question, and answer\ntop_10_context = np.array(context_seq)[:10]\ntop_10_question = np.array(question_seq)[:10]\ntop_10_answer = np.array(answer_seq)[:50]\n\nprint(\"Top 10 Tokenized and Padded Context Sequences:\")\nprint(top_10_context)\n\nprint(\"\\nTop 10 Tokenized and Padded Question Sequences:\")\nprint(top_10_question)\n\nprint(\"\\nTop 10 Tokenized and Padded Answer Sequences:\")\nprint(top_10_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:32.805136Z","iopub.execute_input":"2025-04-22T18:01:32.805770Z","iopub.status.idle":"2025-04-22T18:01:32.814071Z","shell.execute_reply.started":"2025-04-22T18:01:32.805752Z","shell.execute_reply":"2025-04-22T18:01:32.813256Z"}},"outputs":[{"name":"stdout","text":"Top 10 Tokenized and Padded Context Sequences:\n[[4093    2 1774 ...    0    0    0]\n [  22    2  932 ...    0    0    0]\n [  22    2  932 ...    0    0    0]\n ...\n [  44  214   29 ...    0    0    0]\n [  22    2  632 ...    0    0    0]\n [  22    2  632 ...    0    0    0]]\n\nTop 10 Tokenized and Padded Question Sequences:\n[[  132    58    35     2  1774 12913     2 10492  4254   755    68    51\n  19271    21  8997 20979     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [   16   109    35     2 19272    83    14     2   568 23479     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [   16    86     2   109  1192     6 13782     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [  158    10    11   421   422     3  8948     4 21310   941     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [   64    38   683   333    31    43  2161     6    28   277    13     7\n   6541  2117     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [   16    10     2    93     3     2  2007     6  3176   683  6541 13783\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [   71    10  1043    14 10672  6541    54 23480     0     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [   16    10    40   683    54    21    29    43  2161     6    28   277\n     13     7  6541  2117     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [ 1853     2   632  1999    25    40    10    19  3676 20980     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]\n [   16    10     2   326  5583  8178    19  3676 20980     0     0     0\n      0     0     0     0     0     0     0     0     0     0     0     0\n      0     0     0     0     0     0     0     0     0]]\n\nTop 10 Tokenized and Padded Answer Sequences:\n[[   8 2119    9 ...    0    0    0]\n [   8 5859    9 ...    0    0    0]\n [   8    2  207 ...    0    0    0]\n ...\n [   8   11 1319 ...    0    0    0]\n [   8  193    9 ...    0    0    0]\n [   8 9703 3678 ...    0    0    0]]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# !wget http://nlp.stanford.edu/data/glove.6B.zip\n# !unzip glove.6B.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:32.814897Z","iopub.execute_input":"2025-04-22T18:01:32.815186Z","iopub.status.idle":"2025-04-22T18:01:32.828363Z","shell.execute_reply.started":"2025-04-22T18:01:32.815168Z","shell.execute_reply":"2025-04-22T18:01:32.827534Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import numpy as np\n\ndef load_glove_embeddings(glove_path, word_index, embedding_dim=100):\n    embeddings_index = {}\n\n    # Load GloVe file line by line\n    with open(glove_path, encoding='utf-8') as f:\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n\n    print(f\"Loaded {len(embeddings_index)} word vectors from GloVe.\")\n\n    # Initialize embedding matrix with zeros\n    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n\n    # Fill embedding matrix\n    for word, i in word_index.items():\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n\n    return embedding_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:32.829274Z","iopub.execute_input":"2025-04-22T18:01:32.829478Z","iopub.status.idle":"2025-04-22T18:01:32.849034Z","shell.execute_reply.started":"2025-04-22T18:01:32.829463Z","shell.execute_reply":"2025-04-22T18:01:32.848156Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"glove_path = '/kaggle/input/glove6b100dtxt/glove.6B.100d.txt'  # or your full path\nembedding_dim = 100\n\nembedding_matrix = load_glove_embeddings(glove_path, tokenizer.word_index, embedding_dim)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:32.850041Z","iopub.execute_input":"2025-04-22T18:01:32.850342Z","iopub.status.idle":"2025-04-22T18:01:41.811672Z","shell.execute_reply.started":"2025-04-22T18:01:32.850325Z","shell.execute_reply":"2025-04-22T18:01:41.810939Z"}},"outputs":[{"name":"stdout","text":"Loaded 400000 word vectors from GloVe.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"Embedding matrix shape:\", embedding_matrix.shape)\nvocab_size = embedding_matrix.shape[0]  # <- safest way\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:41.812660Z","iopub.execute_input":"2025-04-22T18:01:41.812963Z","iopub.status.idle":"2025-04-22T18:01:41.817236Z","shell.execute_reply.started":"2025-04-22T18:01:41.812943Z","shell.execute_reply":"2025-04-22T18:01:41.816532Z"}},"outputs":[{"name":"stdout","text":"Embedding matrix shape: (26965, 100)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n\n# === SHARED EMBEDDING ===\nembedding_layer = Embedding(\n    input_dim=vocab_size,\n    output_dim=embedding_dim,\n    weights=[embedding_matrix],\n    input_length=max_question_len,\n    trainable=False,\n    name='shared_embedding'\n)\n\n# === ENCODER ===\nencoder_inputs = Input(shape=(max_question_len,), name='encoder_input')\nencoder_embed = embedding_layer(encoder_inputs)\n\nencoder_lstm = LSTM(256, return_state=True, name='encoder_lstm')\n_, state_h, state_c = encoder_lstm(encoder_embed)\n\n# === DECODER ===\ndecoder_inputs = Input(shape=(max_answer_len,), name='decoder_input')\ndecoder_embed = embedding_layer(decoder_inputs)  # Share same embedding\n\ndecoder_lstm = LSTM(256, return_sequences=True, name='decoder_lstm')\ndecoder_outputs = decoder_lstm(decoder_embed, initial_state=[state_h, state_c])\n\ndecoder_dense = Dense(vocab_size, activation='softmax', name='output_dense')\ndecoder_outputs = decoder_dense(decoder_outputs)\n\n# === FULL MODEL ===\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.compile(\n    optimizer='adam',\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:52.666358Z","iopub.execute_input":"2025-04-22T18:01:52.666644Z","iopub.status.idle":"2025-04-22T18:01:52.796042Z","shell.execute_reply.started":"2025-04-22T18:01:52.666623Z","shell.execute_reply":"2025-04-22T18:01:52.795420Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ decoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33\u001b[0m)             │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ shared_embedding          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │      \u001b[38;5;34m2,696,500\u001b[0m │ encoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │ decoder_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │        \u001b[38;5;34m365,568\u001b[0m │ shared_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n│                           │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]     │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_lstm (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m365,568\u001b[0m │ shared_embedding[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n│                           │                        │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],    │\n│                           │                        │                │ encoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ output_dense (\u001b[38;5;33mDense\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m26965\u001b[0m)      │      \u001b[38;5;34m6,930,005\u001b[0m │ decoder_lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ decoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span>)             │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ shared_embedding          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,696,500</span> │ encoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │ decoder_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ encoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ shared_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n│                           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]     │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ decoder_lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │ shared_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│                           │                        │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],    │\n│                           │                        │                │ encoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]     │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ output_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26965</span>)      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,930,005</span> │ decoder_lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m10,357,641\u001b[0m (39.51 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">10,357,641</span> (39.51 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,661,141\u001b[0m (29.22 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,661,141</span> (29.22 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,696,500\u001b[0m (10.29 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,696,500</span> (10.29 MB)\n</pre>\n"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"import numpy as np\n\ndecoder_target_data = np.zeros_like(answer_seq)\ndecoder_target_data[:, :-1] = answer_seq[:, 1:]\ndecoder_target_data[:, -1] = 0  # optional: pad the last token with 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:56.751013Z","iopub.execute_input":"2025-04-22T18:01:56.751356Z","iopub.status.idle":"2025-04-22T18:01:56.756170Z","shell.execute_reply.started":"2025-04-22T18:01:56.751316Z","shell.execute_reply":"2025-04-22T18:01:56.755329Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"from tensorflow.keras.callbacks import ModelCheckpoint\n\n# Create a checkpoint callback\ncheckpoint = ModelCheckpoint(\n    filepath=\"seq2seq_checkpoint.keras\",  # required .keras format\n    save_best_only=False,              # or True if you're using validation loss to pick the best one\n    save_weights_only=False,           # save full model, not just weights\n    verbose=1\n)\n\n# Now pass the callback to model.fit\nhistory = model.fit(\n    [question_seq, answer_seq],\n    np.expand_dims(decoder_target_data, -1),\n    batch_size=64,\n    epochs=15,\n    validation_split=0.1,\n    callbacks=[checkpoint]  # <-- this is the key line\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:01:56.938396Z","iopub.execute_input":"2025-04-22T18:01:56.938908Z","iopub.status.idle":"2025-04-22T18:06:28.690122Z","shell.execute_reply.started":"2025-04-22T18:01:56.938881Z","shell.execute_reply":"2025-04-22T18:06:28.689503Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1745344921.564305     105 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.8453 - loss: 3.9930\nEpoch 1: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 110ms/step - accuracy: 0.8455 - loss: 3.9797 - val_accuracy: 0.8850 - val_loss: 1.0115\nEpoch 2/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step - accuracy: 0.8987 - loss: 0.8386 \nEpoch 2: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 109ms/step - accuracy: 0.8987 - loss: 0.8386 - val_accuracy: 0.8916 - val_loss: 0.9884\nEpoch 3/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.9050 - loss: 0.7888\nEpoch 3: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 110ms/step - accuracy: 0.9050 - loss: 0.7889 - val_accuracy: 0.8940 - val_loss: 0.9805\nEpoch 4/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.9058 - loss: 0.7642\nEpoch 4: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 112ms/step - accuracy: 0.9058 - loss: 0.7642 - val_accuracy: 0.8956 - val_loss: 0.9782\nEpoch 5/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9063 - loss: 0.7458\nEpoch 5: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 115ms/step - accuracy: 0.9063 - loss: 0.7458 - val_accuracy: 0.8959 - val_loss: 0.9799\nEpoch 6/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.9072 - loss: 0.7257\nEpoch 6: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 118ms/step - accuracy: 0.9072 - loss: 0.7257 - val_accuracy: 0.8961 - val_loss: 0.9813\nEpoch 7/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9074 - loss: 0.7125\nEpoch 7: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 117ms/step - accuracy: 0.9074 - loss: 0.7125 - val_accuracy: 0.8964 - val_loss: 0.9822\nEpoch 8/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9075 - loss: 0.7018\nEpoch 8: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 115ms/step - accuracy: 0.9075 - loss: 0.7018 - val_accuracy: 0.8965 - val_loss: 0.9874\nEpoch 9/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 105ms/step - accuracy: 0.9081 - loss: 0.6919\nEpoch 9: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 115ms/step - accuracy: 0.9081 - loss: 0.6920 - val_accuracy: 0.8968 - val_loss: 0.9891\nEpoch 10/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9078 - loss: 0.7006\nEpoch 10: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 115ms/step - accuracy: 0.9078 - loss: 0.7007 - val_accuracy: 0.8979 - val_loss: 0.9876\nEpoch 11/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 108ms/step - accuracy: 0.9092 - loss: 0.6751\nEpoch 11: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 117ms/step - accuracy: 0.9092 - loss: 0.6752 - val_accuracy: 0.8982 - val_loss: 0.9931\nEpoch 12/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9111 - loss: 0.6511\nEpoch 12: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 116ms/step - accuracy: 0.9111 - loss: 0.6512 - val_accuracy: 0.8978 - val_loss: 0.9972\nEpoch 13/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.9089 - loss: 0.6569\nEpoch 13: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 116ms/step - accuracy: 0.9089 - loss: 0.6569 - val_accuracy: 0.8980 - val_loss: 0.9985\nEpoch 14/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9098 - loss: 0.6422\nEpoch 14: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 116ms/step - accuracy: 0.9098 - loss: 0.6423 - val_accuracy: 0.8984 - val_loss: 1.0014\nEpoch 15/15\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.9100 - loss: 0.6318\nEpoch 15: saving model to seq2seq_checkpoint.keras\n\u001b[1m155/155\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 116ms/step - accuracy: 0.9100 - loss: 0.6319 - val_accuracy: 0.8984 - val_loss: 1.0034\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"encoder_model = Model(encoder_inputs, [state_h, state_c])\nlatent_dim = 256  # hidden state size\n\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\n\n\n# Set up the embedding, LSTM and Dense layers again for inference\ndecoder_lstm_infer = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_output, state_h_out, state_c_out = decoder_lstm_infer(decoder_embed, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n\ndecoder_dense_infer = Dense(vocab_size, activation='softmax')\ndecoder_output_infer = decoder_dense_infer(decoder_output)\n\n\n# Decoder model to generate next tokens\ndecoder_model = Model([decoder_inputs] + [decoder_state_input_h, decoder_state_input_c],\n                      [decoder_output_infer, state_h_out, state_c_out])\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:28.691503Z","iopub.execute_input":"2025-04-22T18:06:28.691735Z","iopub.status.idle":"2025-04-22T18:06:28.748860Z","shell.execute_reply.started":"2025-04-22T18:06:28.691718Z","shell.execute_reply":"2025-04-22T18:06:28.748249Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def generate_answer_from_question(question_text, tokenizer, encoder_model, decoder_model, max_answer_len):\n    # Step 1: Tokenize and pad the input question\n    new_question_seq = tokenizer.texts_to_sequences([question_text])\n    new_question_seq = pad_sequences(new_question_seq, maxlen=max_question_len, padding='post')\n\n\n    # Step 2: Encode the question\n    states_value = encoder_model.predict(new_question_seq)\n\n\n    # Step 3: Prepare the <start> token input\n    start_token = tokenizer.word_index[\"<SOS>\"]\n    target_seq = np.zeros((1, 1), dtype='int32')\n    target_seq[0, 0] = start_token\n\n\n    # Step 4: Decode the answer word by word\n    answer = []\n    for _ in range(max_answer_len):\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n\n        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n        sampled_word = tokenizer.index_word.get(sampled_token_index, \"<unk>\")\n        answer.append(sampled_word)\n\n\n        # Break on <end> or <EOS>\n        if sampled_word in [\"<end>\", \"<EOS>\"]:\n            break\n\n\n        # Prepare next token and update decoder state\n        target_seq[0, 0] = sampled_token_index\n        states_value = [h, c]\n\n\n    # Clean output (remove <start> and <end> tokens if present)\n    filtered_answer = [w for w in answer if w not in [\"<start>\", \"<end>\", \"<SOS>\", \"<EOS>\"]]\n    return ' '.join(filtered_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:28.749663Z","iopub.execute_input":"2025-04-22T18:06:28.749877Z","iopub.status.idle":"2025-04-22T18:06:28.756537Z","shell.execute_reply.started":"2025-04-22T18:06:28.749861Z","shell.execute_reply":"2025-04-22T18:06:28.755716Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"question = \"Which year did the USSR cancel the N1 rocket program?\"\ngenerated = generate_answer_from_question(\n    question,\n    tokenizer=tokenizer,\n    encoder_model=encoder_model,\n    decoder_model=decoder_model,\n    max_answer_len=max_answer_len\n)\n\n\nprint(\"Q:\", question)\nprint(\"Generated A:\", generated)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:28.758632Z","iopub.execute_input":"2025-04-22T18:06:28.758870Z","iopub.status.idle":"2025-04-22T18:06:31.480595Z","shell.execute_reply.started":"2025-04-22T18:06:28.758856Z","shell.execute_reply":"2025-04-22T18:06:31.479991Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 194ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 171ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\nQ: Which year did the USSR cancel the N1 rocket program?\nGenerated A: Carroll's Carroll's 218th 218th 218th Bond quarter Examination Examination ψιλά ψιλά ψιλά ψιλά ψιλά ψιλά ψιλά exposure thousands restricted raises 기독교도 annexing antibody Kumba Kumba Kumba values Doolittle Black\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"!zip seq2seq_model.zip seq2seq_checkpoint.keras\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:31.481325Z","iopub.execute_input":"2025-04-22T18:06:31.481584Z","iopub.status.idle":"2025-04-22T18:06:37.522236Z","shell.execute_reply.started":"2025-04-22T18:06:31.481566Z","shell.execute_reply":"2025-04-22T18:06:37.521127Z"}},"outputs":[{"name":"stdout","text":"  adding: seq2seq_checkpoint.keras (deflated 15%)\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:37.523576Z","iopub.execute_input":"2025-04-22T18:06:37.523894Z","iopub.status.idle":"2025-04-22T18:06:37.530362Z","shell.execute_reply.started":"2025-04-22T18:06:37.523856Z","shell.execute_reply":"2025-04-22T18:06:37.529512Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x791ef435f7d0>"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom IPython.display import FileLink\n\n# Option 1: Direct link (simple)\nFileLink('seq2seq_model.zip')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:37.531261Z","iopub.execute_input":"2025-04-22T18:06:37.532019Z","iopub.status.idle":"2025-04-22T18:06:37.565515Z","shell.execute_reply.started":"2025-04-22T18:06:37.532001Z","shell.execute_reply":"2025-04-22T18:06:37.564903Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/seq2seq_model.zip","text/html":"<a href='seq2seq_model.zip' target='_blank'>seq2seq_model.zip</a><br>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n\ndecoder_input_single = Input(shape=(1,), name=\"decoder_input_token\")\n\n# 2. Decoder state inputs\ndecoder_input_h = Input(shape=(256,), name=\"decoder_h\")\ndecoder_input_c = Input(shape=(256,), name=\"decoder_c\")\n\n# 3. Shared embedding layer\ndecoder_embed_single = embedding_layer(decoder_input_single)\n\n# 4. LSTM returns all 3 outputs (wrapped, no unpacking!)\ndecoder_lstm_out = decoder_lstm(\n    decoder_embed_single,\n    initial_state=[decoder_input_h, decoder_input_c]\n)\n\n# Use indexing to access outputs safely\ndecoder_output_tokens = decoder_lstm_out[0]\ndecoder_output_h = decoder_lstm_out[1]\ndecoder_output_c = decoder_lstm_out[2]\n\n# 5. Dense output layer\ndecoder_softmax = decoder_dense(decoder_output_tokens)\n\n# 6. Final decoder inference model\ndecoder_model = Model(\n    inputs=[decoder_input_single, decoder_input_h, decoder_input_c],\n    outputs=[decoder_softmax, decoder_output_h, decoder_output_c]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:37.566328Z","iopub.execute_input":"2025-04-22T18:06:37.566542Z","iopub.status.idle":"2025-04-22T18:06:37.580724Z","shell.execute_reply.started":"2025-04-22T18:06:37.566526Z","shell.execute_reply":"2025-04-22T18:06:37.580123Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"def decode_sequence(question_text):\n    # Encode question\n    seq = tokenizer.texts_to_sequences([question_text])\n    seq = pad_sequences(seq, maxlen=max_question_len, padding='post')\n\n    states_value = encoder_model.predict(seq)\n\n    # Start with <SOS>\n    target_seq = np.zeros((1, 1), dtype='int32')\n    target_seq[0, 0] = tokenizer.word_index[\"<SOS>\"]\n\n    decoded_sentence = []\n    stop_condition = False\n\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n\n        # ✅ fixed index: shape is (1, vocab_size)\n        sampled_token_index = np.argmax(output_tokens[0])\n        sampled_word = tokenizer.index_word.get(sampled_token_index, \"<unk>\")\n\n        if sampled_word == \"<EOS>\" or len(decoded_sentence) > max_answer_len:\n            stop_condition = True\n        else:\n            decoded_sentence.append(sampled_word)\n\n        # Feed the next token\n        target_seq[0, 0] = sampled_token_index\n        states_value = [h, c]\n\n    return ' '.join(decoded_sentence)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:37.581764Z","iopub.execute_input":"2025-04-22T18:06:37.582096Z","iopub.status.idle":"2025-04-22T18:06:37.619583Z","shell.execute_reply.started":"2025-04-22T18:06:37.582072Z","shell.execute_reply":"2025-04-22T18:06:37.618939Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Decoder inference model\ndecoder_state_input_h = Input(shape=(256,))\ndecoder_state_input_c = Input(shape=(256,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n\n# Apply the embedding layer to the decoder inputs\ndecoder_embed2 = embedding_layer(decoder_inputs)\n\n# Apply the LSTM layer to the embedded decoder input and states\ndecoder_lstm_output = decoder_lstm(decoder_embed2, initial_state=decoder_states_inputs)\n\n# The output from the LSTM is a tuple: (output_sequence, state_h, state_c)\ndecoder_outputs2 = decoder_lstm_output[0]\nstate_h2 = decoder_lstm_output[1]\nstate_c2 = decoder_lstm_output[2]\n\n# Apply the dense layer\ndecoder_outputs2 = decoder_dense(decoder_outputs2)\n\n# Define the decoder model\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs2] + [state_h2, state_c2]\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:12:45.979868Z","iopub.execute_input":"2025-04-22T18:12:45.980195Z","iopub.status.idle":"2025-04-22T18:12:45.990579Z","shell.execute_reply.started":"2025-04-22T18:12:45.980173Z","shell.execute_reply":"2025-04-22T18:12:45.990038Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"def decode_sequence(input_sequence):\n    # Step 1: Process the input sequence (e.g., tokenization, padding)\n    target_seq = process_input(input_sequence)  # Shape: (1, 1), e.g., one token at a time\n    \n    # Step 2: Initialize states\n    states_value = [initial_state_h, initial_state_c]  # State from the encoder\n    \n    stop_condition = False\n    decoded_sentence = ''\n    \n    while not stop_condition:\n        # Predict the next token and the new states\n        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n        \n        # Extract the most probable token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_token_index[sampled_token_index]\n        \n        # Append the sampled token to the decoded sentence\n        decoded_sentence += sampled_token + ' '\n        \n        # Update the target sequence (next token) and states\n        target_seq = np.zeros((1, 1))  # Reset to zeros (for the next token prediction)\n        target_seq[0, 0] = sampled_token_index\n        \n        states_value = [h, c]  # Update the states for the next time step\n        \n        # Define a stop condition (e.g., max length or end token)\n        if sampled_token == '<end>' or len(decoded_sentence) > max_decoder_seq_length:\n            stop_condition = True\n    \n    return decoded_sentence\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:27:00.031907Z","iopub.execute_input":"2025-04-22T18:27:00.032501Z","iopub.status.idle":"2025-04-22T18:27:00.038348Z","shell.execute_reply.started":"2025-04-22T18:27:00.032480Z","shell.execute_reply":"2025-04-22T18:27:00.037526Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Select the last 1000 records\nlast_1000_records = df.tail(1000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:27:00.271968Z","iopub.execute_input":"2025-04-22T18:27:00.272541Z","iopub.status.idle":"2025-04-22T18:27:00.276338Z","shell.execute_reply.started":"2025-04-22T18:27:00.272509Z","shell.execute_reply":"2025-04-22T18:27:00.275712Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Extract questions and corresponding answers\nquestions = last_1000_records['question'].tolist()\nground_truth_answers = last_1000_records['answer_text'].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:27:00.503927Z","iopub.execute_input":"2025-04-22T18:27:00.504218Z","iopub.status.idle":"2025-04-22T18:27:00.508330Z","shell.execute_reply.started":"2025-04-22T18:27:00.504198Z","shell.execute_reply":"2025-04-22T18:27:00.507530Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"generated_answers = []\nfor question in questions:\n    generated_answer = decode_sequence(question)\n    generated_answers.append(generated_answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:27:00.744436Z","iopub.execute_input":"2025-04-22T18:27:00.744981Z","iopub.status.idle":"2025-04-22T18:27:00.766013Z","shell.execute_reply.started":"2025-04-22T18:27:00.744960Z","shell.execute_reply":"2025-04-22T18:27:00.764965Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/2291432662.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mgenerated_answers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mgenerated_answer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mgenerated_answers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated_answer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_31/2042049244.py\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_sequence)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# Step 1: Process the input sequence (e.g., tokenization, padding)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtarget_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_sequence\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Shape: (1, 1), e.g., one token at a time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Step 2: Initialize states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'process_input' is not defined"],"ename":"NameError","evalue":"name 'process_input' is not defined","output_type":"error"}],"execution_count":42},{"cell_type":"code","source":"for i in range(10):  # Display first 10 results for verification\n    print(f\"Q: {questions[i]}\")\n    print(f\"Ground Truth: {ground_truth_answers[i]}\")\n    print(f\"Generated: {generated_answers[i]}\")\n    print()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:12:53.105715Z","iopub.execute_input":"2025-04-22T18:12:53.106329Z","iopub.status.idle":"2025-04-22T18:12:53.122347Z","shell.execute_reply.started":"2025-04-22T18:12:53.106305Z","shell.execute_reply":"2025-04-22T18:12:53.121254Z"}},"outputs":[{"name":"stdout","text":"Q: How many regional MPs have argued for the importance of Plymouth's train service?\nGround Truth: <SOS> three <EOS>\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/587382028.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Q: {questions[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Ground Truth: {ground_truth_answers[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Generated: {generated_answers[i]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"],"ename":"IndexError","evalue":"list index out of range","output_type":"error"}],"execution_count":38},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\n\nbleu_scores = []\nfor i in range(1000):\n    reference = ground_truth_answers[i].split()  # Reference answer\n    hypothesis = generated_answers[i].split()  # Generated answer\n    bleu_score = sentence_bleu([reference], hypothesis)\n    bleu_scores.append(bleu_score)\n\nprint(f\"Average BLEU Score: {np.mean(bleu_scores)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:37.677482Z","iopub.status.idle":"2025-04-22T18:06:37.677728Z","shell.execute_reply.started":"2025-04-22T18:06:37.677613Z","shell.execute_reply":"2025-04-22T18:06:37.677623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(bleu_scores)\nplt.title(\"BLEU Scores for Generated Answers\")\nplt.xlabel(\"Sample Index\")\nplt.ylabel(\"BLEU Score\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T18:06:37.679159Z","iopub.status.idle":"2025-04-22T18:06:37.679428Z","shell.execute_reply.started":"2025-04-22T18:06:37.679306Z","shell.execute_reply":"2025-04-22T18:06:37.679316Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}