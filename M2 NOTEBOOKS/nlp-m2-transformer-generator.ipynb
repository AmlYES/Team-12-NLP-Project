{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom transformers import BertTokenizerFast\n\n!pip install datasets\nfrom datasets import load_dataset\n\n# Load SQuAD dataset\ndataset = load_dataset(\"squad\", split=\"train\")\n\n# Step 1: Add a new column for context length\ndataset = dataset.map(lambda x: {\"context_length\": len(x[\"context\"].split())})\ndataset = dataset.map(lambda x: {\"question_length\": len(x[\"question\"].split())})\n\n\n# Step 2: Sort by the new column\nsorted_dataset = dataset.sort(\"context_length\")\n\n# Step 3: Select the shortest 10k rows (or whatever range you need)\nsubset = sorted_dataset.select(range(11000))  # You can change this to 5000, 20000, etc.\ndf = subset.to_pandas()\n\n# Initialize tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:56:35.315157Z","iopub.execute_input":"2025-04-22T16:56:35.315370Z","iopub.status.idle":"2025-04-22T16:57:22.359311Z","shell.execute_reply.started":"2025-04-22T16:56:35.315353Z","shell.execute_reply":"2025-04-22T16:57:22.358657Z"}},"outputs":[{"name":"stderr","text":"2025-04-22 16:56:40.889136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745341001.074450      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745341001.135360      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed fsspec-2024.12.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.62k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18edbf66cc784450b9a0cafd3e2e4cb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f9eb0bbb4a464cadf3db36b07703a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607c8027f7ca42e29df0f3a37ed406d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7dd2d511dc9b4c369f066ea535ccbb30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6219780d0d6f4ca1a67900682fe4eee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69b6aec64db243938af33253b41ed373"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97ce8abc1f344524940e1b00c5d723d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad62aa3cb72e4006aa6c07f5547aba4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cc38d261e6d4925b26aa2b413d1e86a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d8d6dcab312469cb957babeb55b2e69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30e4ea079b3f448c8588f4597fe9fb5f"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"def prepare_qa_data_encoder_decoder(df, tokenizer, max_len=384):\n    context_ids = []\n    context_masks = []\n    question_ids = []\n    question_masks = []\n    start_positions = []\n    end_positions = []\n\n    for i, row in df.iterrows():\n        question = row[\"question\"]\n        context = row[\"context\"]\n        answer_text = row[\"answers\"][\"text\"][0]\n        answer_start = row[\"answers\"][\"answer_start\"][0]\n        answer_end = answer_start + len(answer_text)\n\n        # Tokenize context and question separately\n        context_encoding = tokenizer(\n            context,\n            return_offsets_mapping=True,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None\n        )\n        \n        question_encoding = tokenizer(\n            question,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=max_len,\n            return_tensors=None\n        )\n\n        offsets = context_encoding[\"offset_mapping\"]\n        start_pos, end_pos = 0, 0\n\n        for idx, (start, end) in enumerate(offsets):\n            if start <= answer_start < end:\n                start_pos = idx\n            if start < answer_end <= end:\n                end_pos = idx\n                break\n\n        context_ids.append(context_encoding[\"input_ids\"])\n        context_masks.append(context_encoding[\"attention_mask\"])\n        question_ids.append(question_encoding[\"input_ids\"])\n        question_masks.append(question_encoding[\"attention_mask\"])\n        start_positions.append(start_pos)\n        end_positions.append(end_pos)\n\n    return {\n        \"context_input_ids\": np.array(context_ids),\n        \"context_attention_mask\": np.array(context_masks),\n        \"question_input_ids\": np.array(question_ids),\n        \"question_attention_mask\": np.array(question_masks),\n        \"start_positions\": np.array(start_positions),\n        \"end_positions\": np.array(end_positions),\n    }\n\ntrain_data = prepare_qa_data_encoder_decoder(df, tokenizer)\n\nprint(\"Context input_ids shape:\", train_data[\"context_input_ids\"].shape)\nprint(\"Question input_ids shape:\", train_data[\"question_input_ids\"].shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:57:22.360666Z","iopub.execute_input":"2025-04-22T16:57:22.361186Z","iopub.status.idle":"2025-04-22T16:57:32.839849Z","shell.execute_reply.started":"2025-04-22T16:57:22.361166Z","shell.execute_reply":"2025-04-22T16:57:32.839060Z"}},"outputs":[{"name":"stdout","text":"Context input_ids shape: (11000, 384)\nQuestion input_ids shape: (11000, 384)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"batch_size = 32\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\n        \"context_input_ids\": train_data[\"context_input_ids\"],\n        \"context_attention_mask\": train_data[\"context_attention_mask\"],\n        \"question_input_ids\": train_data[\"question_input_ids\"],\n        \"question_attention_mask\": train_data[\"question_attention_mask\"]\n    },\n    {\n        \"start_positions\": train_data[\"start_positions\"],\n        \"end_positions\": train_data[\"end_positions\"]\n    }\n))\n\ntrain_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:57:32.840529Z","iopub.execute_input":"2025-04-22T16:57:32.840735Z","iopub.status.idle":"2025-04-22T16:57:33.563907Z","shell.execute_reply.started":"2025-04-22T16:57:32.840719Z","shell.execute_reply":"2025-04-22T16:57:33.563356Z"}},"outputs":[{"name":"stderr","text":"I0000 00:00:1745341053.204670      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1745341053.205421      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim):\n        super().__init__()\n        self.token_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.position_embedding = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=embed_dim)\n        self.sequence_length = sequence_length\n\n    def call(self, inputs):\n        positions = tf.range(start=0, limit=self.sequence_length, delta=1)\n        embedded_tokens = self.token_embedding(inputs)\n        embedded_positions = self.position_embedding(positions)\n        return embedded_tokens + embedded_positions\n\nclass TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.dense_proj = tf.keras.Sequential([\n            tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n            tf.keras.layers.Dense(embed_dim),\n        ])\n        self.layernorm1 = tf.keras.layers.LayerNormalization()\n        self.layernorm2 = tf.keras.layers.LayerNormalization()\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs, mask=None, training=False):\n        attention_output = self.attention(inputs, inputs, attention_mask=mask)\n        out1 = self.layernorm1(inputs + self.dropout(attention_output, training=training))\n        dense_output = self.dense_proj(out1)\n        return self.layernorm2(out1 + self.dropout(dense_output, training=training))\n\nclass CrossAttentionDecoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, dropout_rate=0.1):\n        super().__init__()\n        self.cross_attention = tf.keras.layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = tf.keras.Sequential([\n            tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n            tf.keras.layers.Dense(embed_dim),\n        ])\n        self.layernorm1 = tf.keras.layers.LayerNormalization()\n        self.layernorm2 = tf.keras.layers.LayerNormalization()\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n    def call(self, inputs, encoder_outputs, mask=None, training=False):\n        # Cross attention: question attends to context\n        attention_output = self.cross_attention(\n            query=inputs,\n            key=encoder_outputs,\n            value=encoder_outputs,\n            attention_mask=mask\n        )\n        out1 = self.layernorm1(inputs + self.dropout(attention_output, training=training))\n        dense_output = self.dense_proj(out1)\n        return self.layernorm2(out1 + self.dropout(dense_output, training=training))\n\ndef build_encoder_decoder_qa_model(\n    vocab_size,\n    sequence_length,\n    embed_dim=128,\n    dense_dim=512,\n    num_heads=4,\n    num_encoder_layers=2,\n    num_decoder_layers=1\n):\n    # Context encoder inputs\n    context_input_ids = tf.keras.Input(shape=(sequence_length,), dtype=\"int32\", name=\"context_input_ids\")\n    context_attention_mask = tf.keras.Input(shape=(sequence_length,), dtype=\"int32\", name=\"context_attention_mask\")\n    \n    # Question encoder inputs\n    question_input_ids = tf.keras.Input(shape=(sequence_length,), dtype=\"int32\", name=\"question_input_ids\")\n    question_attention_mask = tf.keras.Input(shape=(sequence_length,), dtype=\"int32\", name=\"question_attention_mask\")\n\n    # Context encoder\n    context_embeddings = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(context_input_ids)\n    context_mask = tf.keras.layers.Lambda(\n        lambda x: tf.cast(x[:, tf.newaxis, tf.newaxis, :], dtype=tf.float32)\n    )(context_attention_mask)\n    \n    context_encoded = context_embeddings\n    for _ in range(num_encoder_layers):\n        context_encoded = TransformerEncoder(embed_dim, dense_dim, num_heads)(context_encoded, context_mask)\n\n    # Question encoder\n    question_embeddings = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(question_input_ids)\n    question_mask = tf.keras.layers.Lambda(\n        lambda x: tf.cast(x[:, tf.newaxis, tf.newaxis, :], dtype=tf.float32)\n    )(question_attention_mask)\n    \n    question_encoded = question_embeddings\n    for _ in range(num_encoder_layers):\n        question_encoded = TransformerEncoder(embed_dim, dense_dim, num_heads)(question_encoded, question_mask)\n\n    # Cross attention decoder\n    cross_attention_mask = tf.keras.layers.Lambda(\n        lambda x: tf.cast(x[0][:, tf.newaxis, tf.newaxis, :], dtype=tf.float32)\n    )([context_attention_mask, question_attention_mask])\n    \n    decoder_output = question_encoded\n    for _ in range(num_decoder_layers):\n        decoder_output = CrossAttentionDecoder(embed_dim, dense_dim, num_heads)(\n            decoder_output, context_encoded, cross_attention_mask\n        )\n\n    # Output layers\n    start_logits = tf.keras.layers.Dense(1)(decoder_output)\n    start_logits = tf.keras.layers.Reshape((sequence_length,), name=\"start_positions\")(start_logits)\n\n    end_logits = tf.keras.layers.Dense(1)(decoder_output)\n    end_logits = tf.keras.layers.Reshape((sequence_length,), name=\"end_positions\")(end_logits)\n\n    model = tf.keras.Model(\n        inputs={\n            \"context_input_ids\": context_input_ids,\n            \"context_attention_mask\": context_attention_mask,\n            \"question_input_ids\": question_input_ids,\n            \"question_attention_mask\": question_attention_mask\n        },\n        outputs={\"start_positions\": start_logits, \"end_positions\": end_logits}\n    )\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:57:33.565302Z","iopub.execute_input":"2025-04-22T16:57:33.565525Z","iopub.status.idle":"2025-04-22T16:57:33.636744Z","shell.execute_reply.started":"2025-04-22T16:57:33.565508Z","shell.execute_reply":"2025-04-22T16:57:33.636214Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n\nmodel = build_encoder_decoder_qa_model(\n    vocab_size=len(tokenizer.vocab),\n    sequence_length=384,\n    embed_dim=256,\n    dense_dim=256,\n    num_heads=4,\n    num_encoder_layers=2,\n    num_decoder_layers=1\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=3e-4),\n    loss={\"start_positions\": loss_fn, \"end_positions\": loss_fn},\n    metrics={\"start_positions\": \"accuracy\", \"end_positions\": \"accuracy\"}\n)\n\nmodel.summary()\n\nhistory = model.fit(\n    train_dataset,\n    epochs=15,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T16:57:33.637319Z","iopub.execute_input":"2025-04-22T16:57:33.637546Z","iopub.status.idle":"2025-04-22T17:22:30.568139Z","shell.execute_reply.started":"2025-04-22T16:57:33.637530Z","shell.execute_reply":"2025-04-22T17:22:30.567408Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional_5\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_5\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ question_input_ids        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ question_attention_mask   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ context_input_ids         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ context_attention_mask    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ -                      │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ positional_embedding_1    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m7,911,936\u001b[0m │ question_input_ids[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ question_attention_ma… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ positional_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m7,911,936\u001b[0m │ context_input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mPositionalEmbedding\u001b[0m)     │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lambda (\u001b[38;5;33mLambda\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ context_attention_mas… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,184,512\u001b[0m │ positional_embedding_… │\n│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,184,512\u001b[0m │ positional_embedding[\u001b[38;5;34m…\u001b[0m │\n│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,184,512\u001b[0m │ transformer_encoder_2… │\n│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,184,512\u001b[0m │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m384\u001b[0m)      │              \u001b[38;5;34m0\u001b[0m │ context_attention_mas… │\n│                           │                        │                │ question_attention_ma… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cross_attention_decoder   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,184,512\u001b[0m │ transformer_encoder_3… │\n│ (\u001b[38;5;33mCrossAttentionDecoder\u001b[0m)   │                        │                │ transformer_encoder_1… │\n│                           │                        │                │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_11 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │            \u001b[38;5;34m257\u001b[0m │ cross_attention_decod… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_10 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │            \u001b[38;5;34m257\u001b[0m │ cross_attention_decod… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ end_positions (\u001b[38;5;33mReshape\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ start_positions (\u001b[38;5;33mReshape\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ question_input_ids        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ question_attention_mask   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ context_input_ids         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ context_attention_mask    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ positional_embedding_1    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,911,936</span> │ question_input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ question_attention_ma… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ positional_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,911,936</span> │ context_input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">PositionalEmbedding</span>)     │                        │                │                        │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_attention_mas… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184,512</span> │ positional_embedding_… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184,512</span> │ positional_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184,512</span> │ transformer_encoder_2… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ transformer_encoder_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184,512</span> │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)      │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_attention_mas… │\n│                           │                        │                │ question_attention_ma… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ cross_attention_decoder   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,184,512</span> │ transformer_encoder_3… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">CrossAttentionDecoder</span>)   │                        │                │ transformer_encoder_1… │\n│                           │                        │                │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ cross_attention_decod… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │            <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ cross_attention_decod… │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ end_positions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n│ start_positions (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,746,946\u001b[0m (82.96 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,746,946</span> (82.96 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m21,746,946\u001b[0m (82.96 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,746,946</span> (82.96 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/15\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1745341076.759931     118 service.cc:148] XLA service 0x2ffaa7a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1745341076.760762     118 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745341076.760803     118 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1745341078.501909     118 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1745341089.369046     118 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 282ms/step - end_positions_accuracy: 0.0187 - end_positions_loss: 4.5470 - loss: 8.9662 - start_positions_accuracy: 0.0442 - start_positions_loss: 4.4193\nEpoch 2/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 283ms/step - end_positions_accuracy: 0.0362 - end_positions_loss: 4.1801 - loss: 8.2715 - start_positions_accuracy: 0.0619 - start_positions_loss: 4.0914\nEpoch 3/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 283ms/step - end_positions_accuracy: 0.0581 - end_positions_loss: 4.0499 - loss: 8.0328 - start_positions_accuracy: 0.0723 - start_positions_loss: 3.9828\nEpoch 4/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 282ms/step - end_positions_accuracy: 0.0854 - end_positions_loss: 3.8766 - loss: 7.6688 - start_positions_accuracy: 0.1010 - start_positions_loss: 3.7922\nEpoch 5/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 284ms/step - end_positions_accuracy: 0.1109 - end_positions_loss: 3.6355 - loss: 7.1856 - start_positions_accuracy: 0.1299 - start_positions_loss: 3.5500\nEpoch 6/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 283ms/step - end_positions_accuracy: 0.1378 - end_positions_loss: 3.4351 - loss: 6.7679 - start_positions_accuracy: 0.1590 - start_positions_loss: 3.3328\nEpoch 7/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 283ms/step - end_positions_accuracy: 0.1750 - end_positions_loss: 3.1965 - loss: 6.3021 - start_positions_accuracy: 0.1918 - start_positions_loss: 3.1056\nEpoch 8/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 283ms/step - end_positions_accuracy: 0.2304 - end_positions_loss: 2.9112 - loss: 5.7204 - start_positions_accuracy: 0.2615 - start_positions_loss: 2.8092\nEpoch 9/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 283ms/step - end_positions_accuracy: 0.3115 - end_positions_loss: 2.5633 - loss: 5.0202 - start_positions_accuracy: 0.3317 - start_positions_loss: 2.4568\nEpoch 10/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 283ms/step - end_positions_accuracy: 0.3945 - end_positions_loss: 2.1815 - loss: 4.2690 - start_positions_accuracy: 0.4065 - start_positions_loss: 2.0875\nEpoch 11/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 283ms/step - end_positions_accuracy: 0.4878 - end_positions_loss: 1.8032 - loss: 3.5520 - start_positions_accuracy: 0.4985 - start_positions_loss: 1.7488\nEpoch 12/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 283ms/step - end_positions_accuracy: 0.5707 - end_positions_loss: 1.4727 - loss: 2.8890 - start_positions_accuracy: 0.5825 - start_positions_loss: 1.4163\nEpoch 13/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 284ms/step - end_positions_accuracy: 0.6429 - end_positions_loss: 1.2050 - loss: 2.3685 - start_positions_accuracy: 0.6522 - start_positions_loss: 1.1635\nEpoch 14/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 283ms/step - end_positions_accuracy: 0.7132 - end_positions_loss: 0.9645 - loss: 1.9172 - start_positions_accuracy: 0.7089 - start_positions_loss: 0.9527\nEpoch 15/15\n\u001b[1m344/344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 284ms/step - end_positions_accuracy: 0.7701 - end_positions_loss: 0.7652 - loss: 1.5440 - start_positions_accuracy: 0.7675 - start_positions_loss: 0.7788\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"def predict_answer_encoder_decoder(question, context, tokenizer, model, max_len=384):\n    # Tokenize context and question separately\n    context_encoding = tokenizer(\n        context,\n        return_tensors='tf',\n        truncation=True,\n        padding='max_length',\n        max_length=max_len,\n        return_offsets_mapping=True\n    )\n    \n    question_encoding = tokenizer(\n        question,\n        return_tensors='tf',\n        truncation=True,\n        padding='max_length',\n        max_length=max_len\n    )\n\n    input_dict = {\n        \"context_input_ids\": context_encoding[\"input_ids\"],\n        \"context_attention_mask\": context_encoding[\"attention_mask\"],\n        \"question_input_ids\": question_encoding[\"input_ids\"],\n        \"question_attention_mask\": question_encoding[\"attention_mask\"]\n    }\n    \n    # Run the model\n    outputs = model.predict(input_dict)\n    \n    # Get predicted positions\n    start_logits = outputs['start_positions'][0]\n    end_logits = outputs['end_positions'][0]\n    \n    start_idx = np.argmax(start_logits)\n    end_idx = np.argmax(end_logits)\n    \n    # Handle edge case\n    if end_idx < start_idx:\n        end_idx = start_idx\n    \n    # Convert to character positions\n    offset_mapping = context_encoding[\"offset_mapping\"][0].numpy()\n    start_char = offset_mapping[start_idx][0]\n    end_char = offset_mapping[end_idx][1]\n    \n    return context[start_char:end_char]\n\n# Test prediction\nquestion = \"Which year did the USSR cancel the N1 rocket program?\"\ncontext = \"Meanwhile, 1976...\"\n\npredicted_answer = predict_answer_encoder_decoder(question, context, tokenizer, model)\nprint(\"Predicted Answer:\", predicted_answer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:22:30.569041Z","iopub.execute_input":"2025-04-22T17:22:30.569309Z","iopub.status.idle":"2025-04-22T17:22:33.140625Z","shell.execute_reply.started":"2025-04-22T17:22:30.569280Z","shell.execute_reply":"2025-04-22T17:22:33.140058Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\nPredicted Answer: ,\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"TRY AGAIN TEST ON  THE WHOLE TEST SET WITHOUT CHANGING THE MODEL ITSELF\n","metadata":{}},{"cell_type":"code","source":"test_dataset = subset.select(range(10000, 11000))  # rows 10000 to 10999 (inclusive)\ntest_df = test_dataset.to_pandas()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:27:58.065049Z","iopub.execute_input":"2025-04-22T17:27:58.065349Z","iopub.status.idle":"2025-04-22T17:27:58.090698Z","shell.execute_reply.started":"2025-04-22T17:27:58.065328Z","shell.execute_reply":"2025-04-22T17:27:58.090017Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n# Prepare the test data similarly to the train data\ntest_data = prepare_qa_data_encoder_decoder(test_df, tokenizer)\n\n# Create the test dataset\ntest_dataset_tf = tf.data.Dataset.from_tensor_slices((\n    {\n        \"context_input_ids\": test_data[\"context_input_ids\"],\n        \"context_attention_mask\": test_data[\"context_attention_mask\"],\n        \"question_input_ids\": test_data[\"question_input_ids\"],\n        \"question_attention_mask\": test_data[\"question_attention_mask\"]\n    },\n    {\n        \"start_positions\": test_data[\"start_positions\"],\n        \"end_positions\": test_data[\"end_positions\"]\n    }\n))\n\ntest_dataset_tf = test_dataset_tf.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n\n# Now let's test prediction on a few examples from the test set\ndef evaluate_on_test_set(test_dataset, tokenizer, model, max_len=384, num_samples=10):\n    for idx, (batch_data, labels) in enumerate(test_dataset.take(num_samples)):\n        context_input_ids = batch_data[\"context_input_ids\"]\n        context_attention_mask = batch_data[\"context_attention_mask\"]\n        question_input_ids = batch_data[\"question_input_ids\"]\n        question_attention_mask = batch_data[\"question_attention_mask\"]\n        \n        # We will predict for the first sample in the batch\n        context = tokenizer.decode(context_input_ids[0], skip_special_tokens=True)\n        question = tokenizer.decode(question_input_ids[0], skip_special_tokens=True)\n        \n        print(f\"\\nTest Sample {idx + 1}:\")\n        print(f\"Question: {question}\")\n        print(f\"Context: {context}\")\n        \n        # Get the predicted answer using the model\n        predicted_answer = predict_answer_encoder_decoder(question, context, tokenizer, model)\n        print(f\"Predicted Answer: {predicted_answer}\")\n\n        # Actual answer (if available)\n        actual_start = labels[\"start_positions\"][0].numpy()\n        actual_end = labels[\"end_positions\"][0].numpy()\n        actual_answer = context[actual_start:actual_end]\n        print(f\"Actual Answer: {actual_answer}\")\n\nevaluate_on_test_set(test_dataset_tf, tokenizer, model, max_len=384, num_samples=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T17:28:27.842450Z","iopub.execute_input":"2025-04-22T17:28:27.842702Z","iopub.status.idle":"2025-04-22T17:28:29.779303Z","shell.execute_reply.started":"2025-04-22T17:28:27.842685Z","shell.execute_reply":"2025-04-22T17:28:29.778671Z"}},"outputs":[{"name":"stdout","text":"\nTest Sample 1:\nQuestion: the instruments used to point out the different corrupt forms looked to see if they were rigidly domestic or what?\nContext: the purpose of these instruments was to address the various forms of corruption ( involving the public sector, the private sector, the financing of political activities, etc. ) whether they had a strictly domestic or also a transnational dimension. to monitor the implementation at national level of the requirements and principles provided in those texts, a monitoring mechanism – the group of states against corruption ( also known as greco ) ( french : groupe d ' etats contre la corruption ) was created.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nPredicted Answer: transnational\nActual Answer: d\n\nTest Sample 2:\nQuestion: when were the last of the six great north faces of the alps climbed?\nContext: the first british mont blanc ascent was in 1788 ; the first female ascent in 1819. by the mid - 1850s swiss mountaineers had ascended most of the peaks and were eagerly sought as mountain guides. edward whymper reached the top of the matterhorn in 1865 ( after seven attempts ), and in 1938 the last of the six great north faces of the alps was climbed with the first ascent of the eiger nordwand ( north face of the eiger ).\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nPredicted Answer: 1938\nActual Answer: \n\nTest Sample 3:\nQuestion: what new shipbuilding method was invented in this period?\nContext: the construction of cathedrals and castles advanced building technology, leading to the development of large stone buildings. ancillary structures included new town halls, houses, bridges, and tithe barns. shipbuilding improved with the use of the rib and plank method rather than the old roman system of mortise and tenon. other improvements to ships included the use of lateen sails and the stern - post rudder, both of which increased the speed at which ships could be sailed.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nPredicted Answer: rib and plank\nActual Answer: an\n\nTest Sample 4:\nQuestion: how many months did the songwriters work on the movie via videoconferencing?\nContext: in the increasingly globalized film industry, videoconferencing has become useful as a method by which creative talent in many different locations can collaborate closely on the complex details of film production. for example, for the 2013 award - winning animated film frozen, burbank - based walt disney animation studios hired the new york city - based husband - and - wife songwriting team of robert lopez and kristen anderson - lopez to write the songs, which required two - hour - long transcontinental videoconferences nearly every weekday for about 14 months.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nPredicted Answer: 14\nActual Answer: \n\nTest Sample 5:\nQuestion: what is at least one german innovation that other countries would later adopt in other wars?\nContext: the quick german victory over the french stunned neutral observers, many of whom had expected a french victory and most of whom had expected a long war. the strategic advantages possessed by the germans were not appreciated outside germany until after hostilities had ceased. other countries quickly discerned the advantages given to the germans by their military system, and adopted many of their innovations, particularly the general staff, universal conscription and highly detailed mobilization systems.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\nPredicted Answer: highly detailed mobilization systems\nActual Answer:  ha\n\nTest Sample 6:\nQuestion: how old was gladstone after the general election in 1892?\nContext: gladstone returned to power after the 1892 general election ; he was 82 years old. victoria objected when gladstone proposed appointing the radical mp henry labouchere to the cabinet, so gladstone agreed not to appoint him. in 1894, gladstone retired and, without consulting the outgoing prime minister, victoria appointed lord rosebery as prime minister. his government was weak, and the following year lord salisbury replaced him. salisbury remained prime minister for the remainder of victoria ' s reign.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nPredicted Answer: 82 years old\nActual Answer: ur\n\nTest Sample 7:\nQuestion: what rulings do american courts rarely follow?\nContext: however, it is important to understand that despite the presence of reception statutes, much of contemporary american common law has diverged significantly from english common law. the reason is that although the courts of the various commonwealth nations are often influenced by each other ' s rulings, american courts rarely follow post - revolution commonwealth rulings unless there is no american ruling on point, the facts and law at issue are nearly identical, and the reasoning is strongly persuasive.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\nPredicted Answer: post - revolution commonwealth rulings\nActual Answer: esenc\n\nTest Sample 8:\nQuestion: since when have surface mount packages for capacitors been commonly in use?\nContext: small, cheap discoidal ceramic capacitors have existed since the 1930s, and remain in widespread use. since the 1980s, surface mount packages for capacitors have been widely used. these packages are extremely small and lack connecting leads, allowing them to be soldered directly onto the surface of printed circuit boards. surface mount components avoid undesirable high - frequency effects due to the leads and simplify automated assembly, although manual handling is made difficult due to their small size.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nPredicted Answer: the 1980s\nActual Answer: c\n\nTest Sample 9:\nQuestion: how many grad students were in oklahoma in 2007?\nContext: in the 2007 – 2008 school year, there were 181, 973 undergraduate students, 20, 014 graduate students, and 4, 395 first - professional degree students enrolled in oklahoma colleges. of these students, 18, 892 received a bachelor ' s degree, 5, 386 received a master ' s degree, and 462 received a first professional degree. this means the state of oklahoma produces an average of 38, 278 degree - holders per completions component ( i. e. july 1, 2007 – june 30, 2008 ). national average is 68, 322 total degrees awarded per completions component.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\nPredicted Answer: 20\nActual Answer:  sc\n\nTest Sample 10:\nQuestion: what did the scribal bureaucracy become?\nContext: the organization of the treasury and chancery were developed under the ottoman empire more than any other islamic government and, until the 17th century, they were the leading organization among all their contemporaries. this organization developed a scribal bureaucracy ( known as \" men of the pen \" ) as a distinct group, partly highly trained ulama, which developed into a professional body. the effectiveness of this professional financial body stands behind the success of many great ottoman statesmen.\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\nPredicted Answer: a professional body\nActual Answer:  o\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}